<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE document PUBLIC "+//IDN docutils.sourceforge.net//DTD Docutils Generic//EN//XML" "http://docutils.sourceforge.net/docs/ref/docutils.dtd">
<!-- Generated by Docutils 0.17.1 -->
<document source="/home/runner/work/godot-docs-cn/godot-docs-cn/godot-docs/docs/tutorials/shaders/advanced_postprocessing.rst" xmlns:c="https://www.sphinx-doc.org/" xmlns:changeset="https://www.sphinx-doc.org/" xmlns:citation="https://www.sphinx-doc.org/" xmlns:cpp="https://www.sphinx-doc.org/" xmlns:index="https://www.sphinx-doc.org/" xmlns:js="https://www.sphinx-doc.org/" xmlns:math="https://www.sphinx-doc.org/" xmlns:py="https://www.sphinx-doc.org/" xmlns:rst="https://www.sphinx-doc.org/" xmlns:std="https://www.sphinx-doc.org/">
    <target refid="doc-advanced-postprocessing"></target>
    <section ids="advanced-post-processing doc-advanced-postprocessing" names="advanced\ post-processing 高级后期处理 doc_advanced_postprocessing">
        <title>高级后期处理</title>
        <section ids="introduction" names="introduction 前言">
            <title>前言</title>
            <paragraph>本教程描述了一种在 Godot 中进行后期处理的高级方法。值得注意的是，它将解释如何编写使用深度缓冲区的后期处理着色器。你应该已经熟悉后期处理，特别是使用<reference internal="True" refuri="custom_postprocessing#doc-custom-postprocessing"><inline classes="std std-ref">自定义后期处理教程</inline></reference>中介绍的方法。</paragraph>
        </section>
        <section ids="full-screen-quad" names="full\ screen\ quad 全屏四边形">
            <title>全屏四边形</title>
            <paragraph>One way to make custom post-processing effects is by using a viewport. However,
                there are two main drawbacks of using a Viewport:</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>无法访问深度缓冲区</paragraph>
                </list_item>
                <list_item>
                    <paragraph>在编辑器中看不到后期处理着色器的效果</paragraph>
                </list_item>
            </enumerated_list>
            <paragraph>To get around the limitation on using the depth buffer, use a <reference internal="True" refuri="../../classes/class_meshinstance3d#class-meshinstance3d"><inline classes="std std-ref">MeshInstance3D</inline></reference>
                with a <reference internal="True" refuri="../../classes/class_quadmesh#class-quadmesh"><inline classes="std std-ref">QuadMesh</inline></reference> primitive. This allows us to use a
                shader and to access the depth texture of the scene. Next, use a vertex shader
                to make the quad cover the screen at all times so that the post-processing
                effect will be applied at all times, including in the editor.</paragraph>
            <paragraph>First, create a new MeshInstance3D and set its mesh to a QuadMesh. This creates
                a quad centered at position <literal>(0, 0, 0)</literal> with a width and height of <literal>1</literal>. Set
                the width and height to <literal>2</literal> and enable <strong>Flip Faces</strong>. Right now, the quad
                occupies a position in world space at the origin. However, we want it to move
                with the camera so that it always covers the entire screen. To do this, we will
                bypass the coordinate transforms that translate the vertex positions through the
                difference coordinate spaces and treat the vertices as if they were already in
                clip space.</paragraph>
            <paragraph>顶点着色器希望在裁剪空间中输出坐标，即从屏幕左侧和底部的 <literal>-1</literal> 到屏幕顶部和右侧的 <literal>1</literal> 的坐标。这就是为什么 QuadMesh 的高度和宽度需要是 <literal>2</literal>。Godot 会在幕后处理从模型到视图空间再到剪辑空间的转换，所以我们需要使 Godot 的转换效果无效。我们通过设置内置 <literal>POSITION</literal> 到我们想要的坐标来做到这一点。<literal>POSITION</literal> 会绕过内置变换，直接设置顶点坐标。</paragraph>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">shader_type spatial;

void vertex() {
  POSITION = vec4(VERTEX, 1.0);
}</literal_block>
            <paragraph>即使有了这样的顶点着色器，这个四边形仍会消失。这是因为视锥剔除的缘故，是在 CPU 上完成的。视锥剔除使用摄像机矩阵和 Mesh 的 AABB 来确定 Mesh 是否可见，然后再传递给 GPU。CPU 不知道我们对顶点做了什么，所以它认为指定的坐标指的是世界坐标，而不是裁剪空间的坐标，这就导致了 Godot 在我们旋转、离开场景中心时对四边形进行剔除。为了防止四边形被剔除，有这么几个选项：</paragraph>
            <enumerated_list enumtype="arabic" prefix="" suffix=".">
                <list_item>
                    <paragraph>将 QuadMesh 作为子节点添加到相机，这样相机就会始终指向它</paragraph>
                </list_item>
                <list_item>
                    <paragraph>在 QuadMesh 中将几何属性 <literal>extra_cull_margin</literal> 设置得尽可能大</paragraph>
                </list_item>
            </enumerated_list>
            <paragraph>第二个选项会确保四边形在编辑器中可见，而第一个选项能够保证即使摄像机移出剔除边缘也它仍可见。你也可以同时使用这两个选项。</paragraph>
        </section>
        <section ids="depth-texture" names="depth\ texture 深度纹理">
            <title>深度纹理</title>
            <paragraph>To read from the depth texture, we first need to create a texture uniform set to the depth buffer
                by using <literal>hint_depth_texture</literal>.</paragraph>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">uniform sampler2D depth_texture : source_color, hint_depth_texture;</literal_block>
            <paragraph>Once defined, the depth texture can be read with the <literal>texture()</literal> function.</paragraph>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">float depth = texture(depth_texture, SCREEN_UV).x;</literal_block>
            <note>
                <paragraph>与访问屏幕纹理类似，访问深度纹理只有在从当前视口读取时才能进行。深度纹理不能从你已经渲染的另一个视口中访问。</paragraph>
            </note>
            <paragraph>The values returned by <literal>depth_texture</literal> are between <literal>0.0</literal> and <literal>1.0</literal> and are nonlinear.
                When displaying depth directly from the <literal>depth_texture</literal>, everything will look almost
                white unless it is very close. This is because the depth buffer stores objects closer
                to the camera using more bits than those further, so most of the detail in depth
                buffer is found close to the camera. In order to make the depth value align with world or
                model coordinates, we need to linearize the value. When we apply the projection matrix to the
                vertex position, the z value is made nonlinear, so to linearize it, we multiply it by the
                inverse of the projection matrix, which in Godot, is accessible with the variable
                <literal>INV_PROJECTION_MATRIX</literal>.</paragraph>
            <paragraph>Firstly, take the screen space coordinates and transform them into normalized device
                coordinates (NDC). NDC run <literal>-1.0</literal> to <literal>1.0</literal> in <literal>x</literal> and <literal>y</literal> directions and
                from <literal>0.0</literal> to <literal>1.0</literal> in the <literal>z</literal> direction when using the Vulkan backend.
                Reconstruct the NDC using <literal>SCREEN_UV</literal> for the <literal>x</literal> and <literal>y</literal> axis, and
                the depth value for <literal>z</literal>.</paragraph>
            <note>
                <paragraph>This tutorial assumes the use of the Vulkan renderer, which uses NDCs with a Z-range
                    of <literal>[0.0, 1.0]</literal>. In contrast, OpenGL uses NDCs with a Z-range of <literal>[-1.0, 1.0]</literal>.</paragraph>
            </note>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">void fragment() {
  float depth = texture(depth_texture, SCREEN_UV).x;
  vec3 ndc = vec3(SCREEN_UV * 2.0 - 1.0, depth);
}</literal_block>
            <paragraph>通过将NDC乘以 <literal>INV_PROJECTION_MATRIX</literal> , 将NDC转换成视图空间. 回顾一下, 视图空间给出了相对于相机的位置, 所以 <literal>z</literal> 值将给我们提供到该点的距离.</paragraph>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">void fragment() {
  ...
  vec4 view = INV_PROJECTION_MATRIX * vec4(ndc, 1.0);
  view.xyz /= view.w;
  float linear_depth = -view.z;
}</literal_block>
            <paragraph>因为摄像机是朝向负的 <literal>z</literal> 方向的, 所以坐标会有一个负的 <literal>z</literal> 值. 为了得到一个可用的深度值, 我们必须否定 <literal>view.z</literal> .</paragraph>
            <paragraph>The world position can be constructed from the depth buffer using the following code. Note
                that the <literal>INV_VIEW_MATRIX</literal> is needed to transform the position from view space into world space, so
                it needs to be passed to the fragment shader with a varying.</paragraph>
            <literal_block force="False" highlight_args="{}" language="glsl" linenos="False" xml:space="preserve">varying mat4 CAMERA;

void vertex() {
  CAMERA = INV_VIEW_MATRIX;
}

void fragment() {
  ...
  vec4 world = CAMERA * INV_PROJECTION_MATRIX * vec4(ndc, 1.0);
  vec3 world_position = world.xyz / world.w;
}</literal_block>
        </section>
        <section ids="an-optimization" names="an\ optimization 优化">
            <title>优化</title>
            <paragraph>你可以使用单个大三角形而不是使用全屏四边形. 解释的原因在 <reference name="这里" refuri="https://michaldrobot.com/2014/04/01/gcn-execution-patterns-in-full-screen-passes">这里</reference><target ids="id1" names="这里" refuri="https://michaldrobot.com/2014/04/01/gcn-execution-patterns-in-full-screen-passes"></target> . 但是, 这种好处非常小, 只有在运行特别复杂的片段着色器时才有用.</paragraph>
            <paragraph>Set the Mesh in the MeshInstance3D to an <reference internal="True" refuri="../../classes/class_arraymesh#class-arraymesh"><inline classes="std std-ref">ArrayMesh</inline></reference>. An
                ArrayMesh is a tool that allows you to easily construct a Mesh from Arrays for
                vertices, normals, colors, etc.</paragraph>
            <paragraph>Now, attach a script to the MeshInstance3D and use the following code:</paragraph>
            <literal_block force="False" language="gdscript" linenos="False" xml:space="preserve">extends MeshInstance3D

func _ready():
  # Create a single triangle out of vertices:
  var verts = PackedVector3Array()
  verts.append(Vector3(-1.0, -1.0, 0.0))
  verts.append(Vector3(-1.0, 3.0, 0.0))
  verts.append(Vector3(3.0, -1.0, 0.0))

  # Create an array of arrays.
  # This could contain normals, colors, UVs, etc.
  var mesh_array = []
  mesh_array.resize(Mesh.ARRAY_MAX) #required size for ArrayMesh Array
  mesh_array[Mesh.ARRAY_VERTEX] = verts #position of vertex array in ArrayMesh Array

  # Create mesh from mesh_array:
  mesh.add_surface_from_arrays(Mesh.PRIMITIVE_TRIANGLES, mesh_array)</literal_block>
            <note>
                <paragraph>The triangle is specified in normalized device coordinates.
                    Recall, NDC run from <literal>-1.0</literal> to <literal>1.0</literal> in both the <literal>x</literal> and <literal>y</literal>
                    directions. This makes the screen <literal>2</literal> units wide and <literal>2</literal> units
                    tall. In order to cover the entire screen with a single triangle, use
                    a triangle that is <literal>4</literal> units wide and <literal>4</literal> units tall, double its
                    height and width.</paragraph>
            </note>
            <paragraph>从上面分配相同的顶点着色器, 所有内容应该看起来完全相同.</paragraph>
            <paragraph>The one drawback to using an ArrayMesh over using a QuadMesh is that the ArrayMesh
                is not visible in the editor because the triangle is not constructed until the scene
                is run. To get around that, construct a single triangle Mesh in a modeling program
                and use that in the MeshInstance3D instead.</paragraph>
            <substitution_definition names="weblate_widget"><reference refuri="https://hosted.weblate.org/engage/godot-engine/zh_CN/?utm_source=widget"><image alt="翻译状态" candidates="{'?': 'https://hosted.weblate.org/widgets/godot-engine/zh_CN/godot-docs/287x66-white.png'}" height="66" uri="https://hosted.weblate.org/widgets/godot-engine/zh_CN/godot-docs/287x66-white.png" width="287"></image></reference></substitution_definition>
        </section>
    </section>
</document>
